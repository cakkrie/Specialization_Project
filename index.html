<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Cornell Tech Design Tech Project</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <div class="left">Cornell Tech</div>
    <div class="center">
      Design Tech<br>
      <span>Project Archive</span>
    </div>
    <div class="right"><a href="#">About</a></div>
  </header>

  <main>
    <div class="project-meta">2025</div>
    <div class="project-title">Design Tech Project</div>
    <div class="course-info">DESIGN 8131 Specialization Project 1</div>
    <div class="project-subtitle">Jinyue (Carrie) Wang, jw2796@cornell.edu | Wing Sze (Sissi) Zheng, wz472@cornell.edu</div>
    <div class="advisor">Advisor<br><strong>Jose Sanchez</strong></div>

    <hr style="border: 0; border-top: 1px solid #333; margin: 32px 0;">

    <div class="section-title">Project Questions</div>
    <div class="section-subtitle">Long Latency</div>


    <!-- Labels -->
    <span class="label outline info">Multi-sensor</span>
    <span class="label outline info">Communication</span>
    <span class="label outline info">Scientific Research</span>
    
    <span class="label outline">Physicalization</span>
    <span class="label outline">Play</span>

    <p>As AI systems take on increasingly complex tasks, the computational resources required to process them grow substantially. This often leads to extended response latency in AI products. Such delays present several challenges for user experience.</p>

    <p>*First, long waiting times test user patience and can lead to frustration, abandonment of tasks, or diminished trust in the system.*Second, interruptions caused by latency disrupt cognitive flow, making it harder for users to maintain continuity in thought and workflow. *Finally, without clear feedback about the system’s current state, users face uncertainty over whether the AI is still processing, stalled, or has failed altogether.</p>

     <!-- Question -->
    <div class="callout">
      <p>How might we reframe latency from a passive waiting period into an active part of the interaction that sustains engagement and confidence in AI product?</p>
    </div>

    <br>
    <div class="section-subtitle">Cognitive difference between stakeholders in AI products</div>

    <!-- Labels -->
    <span class="label outline info">Communication</span>
    <span class="label outline info">Accessibility</span>
    <span class="label outline info">Connectivity</span>
    <span class="label outline info">Scalability</span>
    
    <p>Given the rapid iteration and development of AI models, each serves different disciplines with distinct strengths—for example, code decomposition, visual design, or other specialized tasks.</p>

    <p>While OpenAI has released a leaderboard that is widely accepted in the industry, its benchmarks are primarily designed for ML engineering rather than general users. As a result, users struggle to identify which model best fits their specific needs in this fast-evolving landscape, since different stakeholders prioritize different functions and evaluation dimensions.</p>

    <p>Addressing this gap could help close the cognitive divide between AI products and their consumers. </p>

    <div class="callout">
      <p>How might we design user-friendly evaluation frameworks that help non-expert users identify which AI model best fits their task needs in a rapidly evolving landscape of specialized models?</p>
    </div>

    <br>
    <br>
    <div class="section-title">State of Knowledge of the field</div>
    
    <div class="knowledge-container">
      <div class="knowledge-column">
        <div class="person-header">
          <h3>Carrie Wang</h3>
          <span class="person-role">Design & User Experience</span>
        </div>
        <div class="knowledge-areas">
          <div class="knowledge-item">
            <h4>Mobile/Web UIUX</h4>
            <div class="skill-level expert">Expert</div>
          </div>
          <div class="knowledge-item">
            <h4>AI/ML Concepts</h4>
            <div class="skill-level intermediate">Intermediate</div>
          </div>
          <div class="knowledge-item">
            <h4>Product Management</h4>
            <div class="skill-level intermediate">Intermediate</div>
          </div>
          <div class="knowledge-item">
            <h4>Hardware interaction</h4>
            <div class="skill-level beginner">Beginner</div>
          </div>
        </div>
      </div>
      
      <div class="knowledge-column">
        <div class="person-header">
          <h3>Sissi Zheng</h3>
          <span class="person-role">Technical & Research</span>
        </div>
        <div class="knowledge-areas">
          <div class="knowledge-item">
            <h4>Software/Hardware interaction</h4>
            <div class="skill-level expert">Expert</div>
          </div>
          <div class="knowledge-item">
            <h4>Autonomous Driving System</h4>
            <div class="skill-level intermediate">Intermediate</div>
          </div>
          <div class="knowledge-item">
            <h4>Voice AI Interaction</h4>
            <div class="skill-level intermediate">Intermediate</div>
          </div>
          <div class="knowledge-item">
            <h4>Perception and Localization Algo</h4>
            <div class="skill-level beginner">Beginner</div>
          </div>
        </div>
      </div>
    </div>

    <br>
    <br>
    <div class="section-title">Map of Community of Practice</div>
    <div class="section-subtitle">Long Latency</div>

    <p><b>Current Companies & Solutions:</b>

    <b>Engineering + Infrastructure Optimizations:</b>
    <br>
    Uber: The AI team at Uber’s Development Platform: When handling large-scale operations such as code migration and real-time data querying, this effectively mitigates blocking caused by single-point latency and significantly improves system response speed.
    <br>
    Amazon: offers latency-optimized inference modes to accelerate response times when using LLMs, crucial for improving UX in real-time applications.
    <br>
    Lepton AI: allowing the AI to start responding within 300 milliseconds to a "think-while-speaking" function
    
    <br>
    <br>
    <b>UX / Design Strategies:</b>
    <br>
    Ricky Ho: Fast vs. Slow Path Design: pre-defined flow graphs and intent detection to route simple requests (e.g., weather) through fast paths and complex tasks to LLMs only if needed
    <br>
    Procreator / Duolingo: Infuses empathy into latency UX with friendly micro-copy and humanized messaging
    <br>
    Morgan Stanley / BofA: Embeds UX guidance into workflows & trains users (especially leadership) to manage AI expectations</p>

    <br>
    <div class="section-subtitle">Cognitive Difference</div>

    <p><b>Black Box and Model Evaluation Issues:</b>
    <br>
    Google Vertex Explainable AI: explain the important features of model input and output, and can conduct comparison, evaluation, and prompt optimization for generative models
    <br>
    Rawbot: A toC model comparison platform that showcases the differences between models such as GPT, Claude, and Cohere across multiple tasks
    <br>
    InterpretML: A toolkit to help understand models and enable responsible machine learning
    <br>
    LMArena: Compare answers across top AI models, share your feedback and power our public
    <br>
    <br>
    <b>The Community:</b>
    <ul>
      <li>AI Engineers / Model Developers</li>
      <li>UX / Product Designer</li>
      <li>Infrastructure / DevOps Teams</li>
      <li>Industry Regulators</li>
      <li>End User</li>
    </ul>    
    
    <br>
    <div style="text-align: center;">
      <img src="911.JPG" alt="Project image" width="900">
      <div><small><small><small><em>The Hugging Face Leaderboard</em></small></small></small></div>
    </div>


    <br>
    <br>
    <div class="section-title">Research Methods</div>
    <div class="section-subtitle">General Research</div>

    <p>User trend as AI evolve: Cognitive challenge for AI</p>

    <div class="section-subtitle">For Long Latency</div>

    <ul>
      <li>
        <b>User Observations & Diary Studies</b>
        <ul>
          <li>Observe how users react to latency during real AI tasks.</li>
          <li>Conduct diary studies to capture emotional and cognitive states when latency disrupts flow, and interview users about their feelings.</li>
        </ul>
      </li>
      <li>
        <b>Wizard-of-Oz Prototyping</b>
        <ul>
          <li>Simulate different latency lengths and patterns in a controlled setting.</li>
          <li>Insert alternative feedback (animations, micro-interactions, “play” or physicalization cues) to see how users perceive system state.</li>
        </ul>
      </li>
      <li>
        <b>A/B Testing of Latency Designs</b>
        <ul>
          <li>Compare baseline “blank wait” vs. enriched latency (progressive disclosure, interactive fillers, playful feedback).</li>
          <li>Measure metrics such as task completion, frustration, trust, and perceived usefulness.</li>
        </ul>
      </li>
      <li>
        <b>Physiological / Cognitive Load Measures</b>
        <ul>
          <li>Eye-tracking</li>
        </ul>
      </li>
    </ul>

    <br>
    <div class="section-subtitle">For Cognitive Differences Between Stakeholders in AI Products</div>
    
    <ul>
      <li>
        <b>Stakeholder Interviews</b>
        <ul>
          <li>Conduct semi-structured interviews with:
            <ul>
              <li>AI engineers/researchers (focus: benchmarks, performance metrics)</li>
              <li>Designers/product managers (focus: task applicability, usability)</li>
              <li>Non-expert end users (focus: accessibility, clarity of choice)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <b>Comparative Task Analysis</b>
        <ul>
          <li>Have diverse users attempt the same tasks with different models</li>
          <li>Collect data on ease of selection, perceived “fit,” and satisfaction</li>
        </ul>
      </li>
      <li>
        <b>Card Sorting / Participatory Design Workshops</b>
        <ul>
          <li>Ask users to categorize evaluation criteria (speed, accuracy, creativity, safety, cost)</li>
          <li>Co-create candidate frameworks for model evaluation</li>
        </ul>
      </li>
      <li>
        <b>Survey Research</b>
        <ul>
          <li>Broader-scale data collection to validate which evaluation dimensions are most valued by different groups</li>
        </ul>
      </li>
      <li>
        <b>Prototype Testing of Evaluation Interfaces</b>
        <ul>
          <li>Build mockups of simplified “model selection dashboards” with layered information</li>
          <li>Test whether non-experts can make more confident/accurate model choices</li>
        </ul>
      </li>
    </ul>


    <br>
    <br>
    <div class="section-title">Intent of Purpose</div>
    <div class="section-subtitle">Intent of Purpose</div>
    
    <p>Building on the legacy of Radical Software, which reimagined information as a site of systemic power and resistance, our project proposes a radical prototype that questions dominant UX paradigms in AI systems.</p>

    <div class="knowledge-container">
      <div class="knowledge-column">
        <p>In our imagined future, "waiting" is no longer a resistance to interaction, but becomes one of its core functionalities in which the system can offer distribution of attention, flows of thoughts, collaborative cues, or create a rhythm, making a new narrative on this human-AI co-creation.</p>
      </div>
      
      <div class="knowledge-column">
        <p>As for the second idea— the playground of algorithm blackbox — we believe that the value of a radical prototype lies not in ambiguity of AI, but in actively entering it. It aims to create an open environment where users can play with differences, dismantle positional black boxes, and co-create new relationships with AI.</p>
      </div>
    </div>

    <div class="section-subtitle">Our Project as Critical Distribution</div>

    <div class="knowledge-container">
      <div class="knowledge-column">
        <p>Mainstream AI products currently treat latency as a technical issue to be minimized. In contrast, we reimagine it as an underutilized design opportunity — a potential trigger for playfulness, cognitive rhythm, and reflection. Through this unconventional design approach, we aim to create a critical disruption to the logic of "extreme optimization" in the AI industry by making visible the mechanics of computational labor, model scale, and cognitive cost behind latency.</p>
      </div>
      
      <div class="knowledge-column">
        <p>In the second aspect, we disrupt traditional approach of model evaluation, challenging the engineering- and business-centric assessment logic that dominates the current AI landscape. At the same time, we addresses critical questions like: Who has the authority to define whether a model is "good" or "bad" in their decispline? Whose standards are being enforced — and why?</p>
      </div>
    </div>

    <div class="section-subtitle">Our Radical Prototype</div>

    <p>Our radical prototype will be a system, not a product. We are designing a universal latency-adaptive interaction component that mediates between model delay and user engagement across platforms, tasks, and model types. Its purpose is to guide the users with coexisting with latency, to not hide the waiting but make it meaningful. The second idea, we will make a playground accessible across desktop and mobile, inviting casual users, researchers and other stakeholder to explore AI not as black-box but in a decentrualized system.</p>

</body>
</html>
